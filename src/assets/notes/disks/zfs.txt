-- install (ubuntu) --
sudo apt install -y zfsutils-linux

-- check installation --
whereis zfs
sudo fdisk -l

-- quick commands --

**to list disks on ubuntu: **
* i believe you really need to do by-id (seems like a manufacturer name + uuid) or by-uuid, as just using /dev/sda is not reliable on restarts or changing ports!!*
ls -la /dev/disk/by-id

** create pool **
zpool create vol0 /dev/sd[x]  

** delete pool **
sudo zpool destroy [pool name]  


** delete all datasets in a pool **
zfs destroy -r [pool name] 

** check pools list **
zfs list
zpool list -v

** check status **
zpool status

** another command **
zfs get all

** create dataset (note it wants an empty folder (or for it to not exist) and the parents need to be made!, ie cant do datapool/nfsshare/users/abc without datapool/nssshare and datapool/nsfsshare/users first (also dont forget to chown once done, freaking root**
sudo zfs create datapool/nfsshare

** change mountpoint **
(mkdir -p /zfspools/datapool && chown -R $USER:$USER /zfspools/datapool)
zfs set mountpoint=/zfspools/datapool datapool

** change owner of newly create mountpoint to be user level as / is owned by root **
chown -R $USER:$USER /zfspools
// or (actually dont do below)
// chown -R nobody:nogroup /zfspools/datapool

sudo chmod a+rwx /zfspools  (i think same as chmod 777 /zfspools)
sudo chmod 777 /zfspools/datapool
* i dont think chmod is needed *

** import (may need to do this if moving zpool back or for the first time, the -f flag is because data may have changed and zfs knows it!) (also not sure if it inherits the mountpoint, but apparently carries options like if you has share nfs on) **
zpool import -f datapool

** export (do this before removing !)
zpool export datapool

** online (may be needed after import?) **
zpool online ...

** scrub **
sudo zpool scrub [pool name]

** create dataset (ZFS will automatically mount the dataset at /path/to/pool/[dataset name]. (you can do, zfs create -p mypool/some/lower/dir/to/make , and it will make parent datasets for you )
) **
sudo zfs create [pool name]/[dataset name]  

** set quota (so i think you can pref refquota as it disregards snapshots, so you can set a 100gb quota that includes data + snapshots, and 50gb refquota to just mean 50gb limit on the data!)(amount must be >= current size) (quota applies to snapshots/descendants, refquota is just data) **
zfs set quota=10G tank/home/simba
zfs set quota=none tank/home/simba # make sure to change source back to default using inherit command, it goes to local source after changing to none!
sudo zfs inherit -Sr quota gold/y/users/hi
zfs get -r quota gold/y # check with this to see source

** get quota **
zfs get quota tank/home/simba

** You can set a refquota on a dataset that limits the amount of disk space that the dataset can consume. This hard limit does not include disk space that is consumed by descendents. For example, studentA's 10 GB quota is not impacted by space that is consumed by snapshots. **

# zfs set refquota=10g students/studentA
# zfs list -t all -r students
NAME                          USED  AVAIL  REFER  MOUNTPOINT
students                      150M  66.8G    32K  /students
students/studentA             150M  9.85G   150M  /students/studentA
students/studentA@yesterday      0      -   150M  -
# zfs snapshot students/studentA@today
# zfs list -t all -r students
students                      150M  66.8G    32K  /students
students/studentA             150M  9.90G   100M  /students/studentA
students/studentA@yesterday  50.0M      -   150M  -
students/studentA@today          0      -   100M  -

** In this scenario, studentA might reach the refquota (10 GB) hard limit, but studentA can remove files to recover, even if snapshots exist.

In the preceding example, the smaller of the two quotas (10 GB as compared to 20 GB) is displayed in the zfs list output. To view the value of both quotas, use the zfs get command. For example:**

# zfs get refquota,quota students/studentA
NAME               PROPERTY  VALUE              SOURCE
students/studentA  refquota  10G                local
students/studentA  quota     20G                local

** set reservation (also can do refreserv like refquota) **
zfs set reservation=5G tank/home/bill
** get reservation **
zfs get reservation tank/home/bill

** list **
zfs list -t all -r students

** list datasets **
zfs list -r tank/home

** list size **
df -h /tank/home/jeff
du -sh /tank/home

** You can create a "descendent" dataset/filesystem like so: **
sudo zfs create [pool name]/[dataset name]/[descendent filesystem]

** delete dataset (A dataset cannot be destroyed if snapshots or clones of the dataset exist.)**
sudo zfs destroy [pool name]/[dataset name]  

** Set Dataset Record Size (Size should be a value like 16k, 128k, or 1M etc.) (do this for performance, default is 128k) **
sudo zfs set recordsize=[size] pool/dataset/name

** Get Dataset Record Size **
sudo zfs get recordsize pool/dataset/name

** snapshot **
zfs snapshot [pool]/[dataset name]@[snapshot name]  

** list snapshots **
sudo zfs list -t snapshot  

** rename snapshot **
zfs rename [pool]/[dataset]@[old name] [new name]  

** restore snapshot **
zfs rollback -r [pool]/[dataset]@[snapshot name]  

** delete snapshot **
zfs destroy tank/home/cindys@snap1  

** mount everything (This will let you know if your pool or dataset won't mount for some reason, such as the directory not being empty.) **
zfs mount -a

** get mountpoints **
zfs get all | grep mountpoint

** get all properties **
zfs get all tank/home


** set mountpoint **
language-bashsudo zfs set mountpoint=/path/to/mount zpool-name/dataset-name

** deduplication (Deduplication had a massive negative effect on performance for me on spinning disk.)**
sudo zfs set dedup=on zpool-name

** disable dedup **
sudo zfs set dedup=off zpool-name

** list zpools (as opposed to zpool status) (-H suppresses headers, -o allow you to specify properties) **
zpool list
NAME   SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP    HEALTH  ALTROOT
gold   238G  57.7G   180G        -         -     2%    24%  1.00x    ONLINE  -

zpool list -Ho name
gold

** then can crossreference with **
zfs list -Ho name,mountpoint

gold	/zfspools/gold
gold/y	/zfspools/gold/y
gold/y/organizations	/zfspools/gold/y/organizations
gold/y/users	/zfspools/gold/y/users
gold/y/users/hi	/zfspools/gold/y/users/hi
gold/y/users/yo	/zfspools/gold/y/users/yo
gold/z	/zfspools/gold/z
gold/z/users	/zfspools/gold/z/users
gold/z/users/yo	/zfspools/gold/z/users/yo

** or **
zfs list -Ho mountpoint gold

/zfspools/gold


** enable sharenfs (sudo apt install -y nfs-kernel-server), traditional with nfs you can use /etc/exports, but for zfs you can enable it like **
sudo zfs set sharenfs=”on” /tank/nfsshare

// Earlier, I alluded to giving only certain IPs the access. You can do so as following (The ‘rw’ stands for read-write permissions, and that is followed by the range of IPs. Make sure that the port number 111 and 2049 are open on your firewall. If you are using ufw, you can check that by running:):
sudo zfs set sharenfs="rw=@192.168.0.0/24" tank/nfsshare

** Client Side Mounting
/* This will mount the NFS share on /mnt folder but you could have just as easily picked any other mount point of your choice. Once the share is created, you can mount it on your client machine, by running the command: **/
mount -t nfs  server.ip:/tank/nfsshare /mnt


** stats **
zpool iostat

** with virtual layout **
zpool iostat -v

** stats by pool **
zpool iostat gold




-- quick intro --
- striped (just merge all disk capacities together, data is striped across all drives for speed, any drive failure or partition loss or data loss or otherwise is pretty fatal.  kinda just do this if you want zfs to be the filesystem of the os or youre that broke)


- mirrored (basically, do this if you just have 2 drives, any more and we can really be doing raidz.  this is as it sounds, mirroring drives A and B so that if one fails, the other has a copy.  you will get half the capacity of the combined drives (the smaller of the 2, so try to use the same size okay))

sudo zpool create -m /zfspools/newpool-name/new-pool-name new-pool-name mirror /dev/disk/by-id/ata-PNY_FF /dev/disk/by-id/ata-SATA-SSD_123 && sudo chown -R $USER:$USER /zfspools/new-pool-name && sudo chmod 777 pioneer

* -m option changes mountpoint, default mount point is /usr/share i think *

- zfs (raidz) (minimum of 3 drives, you will add on to the pool 3 at a time.  this is a combination of striping and mirroring. you get 1 disk that can fail (parity info equivalent to 1 disk). (tbd how much data you get)


- zfs2 (raidz2) (minimum of 4 drives, you will get half of the capacity, so you if you had 4 x 1tb drives, you will get 2 tb of a pool.  the benefit is that you get a bit of striping, though the extra parity will likely slow it down to a normal rate.  you can lose at most 2 of the 4 drives and continue to operate!)
sudo zpool create -m /zfspools/silver silver raidz2 /dev/disk/by-id/... /dev/disk/by-id/...2 /dev/disk/by-id/...3 /dev/disk/by-id/...4


- zfs3 (raidz3) (minimum of 5 drives). (tbd)



-- blah --


Creating a Pool
There are two types of simple storage pools we can create. A striped pool , also called RAID-0 , in which the data is stored in “stripes” across all drives, or a mirrored pool , also called RAID-1 , in which a complete copy of all data is stored separately on each drive. Striped pools are not fault tolerant whereas mirrored pools can survive the failure of one drive. Striped pools have twice the storage capacity of mirrored pools and have better performance than mirrored pools.

To create a striped pool, we run:

sudo zpool create new-pool /dev/sdb /dev/sdc

To create a mirrored pool, we run:

sudo zpool create new-pool mirror /dev/sdb /dev/sdc

A mirrored pool is usually recommended as we’d still be able to access our data if a single drive fails. However, this means that we’ll only get the capacity of a single drive. A striped pool, while giving us the combined storage of all drives, is rarely recommended as we’ll lose all our data if a drive fails. You can also opt for both, or change the designation at a later date if you add more drives to the pool.

The newly created pool is mounted at /new-pool. You can select a different mount point using the -m option:

sudo zpool create -m /usr/share/pool new-pool mirror /dev/sdb /dev/sdc
The newly mounted pool will appear to Ubuntu as any other part of the filesystem.

You can check the status of ZFS pools with:

sudo zpool status

sudo zpool destroy new-pool

sudo zpool status

----------

ls /dev/disk/by-uuid  or by-id
ls /dev/disk

----
sudo zpool clear -nFX WD_1TB
If wont help, try the following commands to remove the invalid zpool:

zpool list -v
sudo zfs unmount WD_1TB
sudo zpool destroy -f WD_1TB
zpool detach WD_1TB disk1s2
zpool remove WD_1TB disk1s2
zpool remove WD_1TB /dev/disk1s2
zpool set cachefile=/etc/zfs/zpool.cache WD_1TB
// Finally if nothing helps, remove the file /etc/zfs/zpool.cache (optionally) and just restart your computer.


sudo zpool scrub gold
sudo zpool scrub -s gold  // after a few minutes, it was maybe 10% way through, which you can check with sudo zpool status -v, then magically it was resolved... a full scrub may have also fixed it but i didnt want to wait

// so for suspended / weird mounts that im trying to destroy i had to shut down the machine and unplug the device and it went away

// tank === british pool || matrix character

// force if device is busy
zfs destroy -f tank/home/ahrens
// if pool has descendents
zfs destroy -r tank/ws
// if pool has indirect descendents and needs a force
zfs destroy -R tank/ws

// to move to another machine, i was able to literally shutdown the other machine (probably another way to detatch a pool) then plug it into another one and run
sudo zpool import -f